{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project #5: Wrangle Report -- \"We Rate Dogs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before beginning any analysis, we need to gather the data that will be needed for the analysis, assess the data visually and programmatically, merge data where appropriate, clean the data, and store the data in a format that can be accessed for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all necessary packages/libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "import matplotlib as plt\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: Code is provided for explanatory purposes only here. Run code in wrangle_act to reproduce results.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of the wrangling data project was to gather data from various sources, merge the data, clean the data, analyze the data, and finally present the data. \n",
    "\n",
    "In the Udacity “We Rate Dogs” data wrangling project, we were first tasked with reading in a cvs data file provided by Udacity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the WeRateDogs Twitter archive data provided by Udacity. \n",
    "twitter_archive = pd.read_csv('twitter-archive-enhanced.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we were tasked with reading in an image file programmatically using the Request library and reading in the images from a website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = (\"https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\")\n",
    "\n",
    "response = requests.get(image_url)\n",
    "with open(os.path.join(os.getcwd(), image_url.split('/')[-1]), mode = 'wb') as file:\n",
    "        file.write(response.content)\n",
    "        \n",
    "#Read in image-prediction text file\n",
    "image_prediction = pd.read_csv('image-predictions.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we were tasked with querying a Twitter API for each tweet in the “We Rate Dogs” Twitter archive and saving it as a JSON text file. Then we were to save each retweet returned as a new line in a text file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Twitter API for each tweet in the Twitter archive and save JSON in a text file.\n",
    "\n",
    "consumer_key = 'HIDDEN'\n",
    "consumer_secret = 'HIDDEN'\n",
    "access_token = 'HIDDEN'\n",
    "access_secret = 'HIDDEN'\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read tweets into dataframe \n",
    "tweets_list = []\n",
    "\n",
    "#Use for loop to append each tweet into tweets_list\n",
    "tweet_file = open('tweet_json.txt', \"r\")\n",
    "\n",
    "for line in tweet_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    " \n",
    "        tweets_list.append(tweet)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "tweet_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe to store tweets\n",
    "tweets_df = pd.DataFrame()\n",
    "\n",
    "#Update column names \n",
    "tweets_df['tweet_id'] = list(map(lambda tweet: tweet['id'], tweets_list))\n",
    "tweets_df['retweet_count'] = list(map(lambda tweet: tweet['retweet_count'], tweets_list))\n",
    "tweets_df['favorite_count'] = list(map(lambda tweet: tweet['favorite_count'], tweets_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Assess Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we gathered all the necessary data for our project, we need to access and familiarize ourselves with the data both visually and programmatically.\n",
    "\n",
    "Visual assessment can uncover data anomalies that will need to be cleaned to be prepped for analysis, such as missing or inconsistent data, data inaccuracy, and tidiness issues.\n",
    "\n",
    "Programmatic allows us dive even deeper to uncover inaccurate data types and missing values (df.info()), exploring descriptive statistics (df.describe()), and identifying duplicate values (df.nunique()).\n",
    "\n",
    "Exploring the data allows us to identify issues that need to be cleaned prior to analyzing the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this analysis, I began by merging all three datasets using the image file as the base file since we were only interested in looking ar records with images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_image = image_prediction.merge(twitter_archive, left_on='tweet_id', right_on='tweet_id', how='left')\n",
    "\n",
    "twitter_image_tweets = twitter_image.merge(tweets_df, left_on='tweet_id', right_on='tweet_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, and as advised, I made a copy of the merged file prior to cleaning any data so that I could always refer back to the original if needed. \n",
    "\n",
    "Next, I began to address data quality issues, beginning with the removal of extra characters that were added to the timestamp field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_image_clean = twitter_image_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_image_clean.timestamp  = tw_image_clean.timestamp.str.strip('+0000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the extra characters, I converted the timestamp field to a datetime data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_image_clean['timestamp'] = pd.to_datetime(tw_image_clean['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I converted all ‘rating_denominators’ to 10 to be consistent since there were some denominators identified in the data as 0 and 170.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_image_clean['rating_denominator'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I filtered out all retweets since we were instructed not included these in our final data file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_image_clean = tw_image_clean.query('retweeted_status_id == \"NaN\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After filtering out retweets, several columns became redundant so I filtered those out as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "tw_image_clean.drop('retweeted_status_id', axis = 1, inplace = True)\n",
    "tw_image_clean.drop('retweeted_status_user_id', axis = 1, inplace = True)\n",
    "tw_image_clean.drop('retweeted_status_timestamp', axis = 1, inplace = True)\n",
    "tw_image_clean.drop('in_reply_to_status_id', axis = 1, inplace = True)\n",
    "tw_image_clean.drop('in_reply_to_user_id', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I addressed the next tidiness issue which was the notation of dog state in four separate columns. To address this issue, I merged these four columns into one dog_stage column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py:3798: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  method=method)\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "tw_image_clean.replace('None', np.nan, inplace=True)\n",
    "tw_image_clean['dog_stage'] = tw_image_clean[['doggo', 'floofer', 'pupper', 'puppo']].fillna('').sum(axis=1).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_image_clean = tw_image_clean.drop(['doggo', 'floofer', 'pupper', 'puppo'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This unveiled a few other data quality issues. The first being that the majority of the data for the dog_stage field was missing. The next being a handful of records fell into buckets outside of the four main dog stages. To clean these records, I used visual assessment to review each record individually and assign it to the correct bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_image_clean[tw_image_clean['dog_stage'] == \"doggofloofer\"]\n",
    "\n",
    "tw_image_clean.drop(tw_image_clean.loc[tw_image_clean['tweet_id'] == 854010172552949760].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I looked at dog names and again found many missing values. In some cases, the dog name appeared in the text, but this was not consistent. In addition, there was no way to know if several images of the same dog appeared or if different dogs appeared with the same name. At this point, I decided I did not have enough information to make any dependable insights so I updated all names with None values to Nan and moved on. I could have extracted some names from text, but this would not have solved for the issues identified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_image_clean.dog_stage = tw_image_clean['dog_stage'].replace('None', np.NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I noticed that text entries where non-dog images had been submitted included the phrase “We only rate dogs” or “I only rate dogs” so I dropped all of these records since these images aligned with non-dog entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchfor = ['We only rate dogs.', 'I only rate dogs.']\n",
    "tw_image_clean = tw_image_clean[~tw_image_clean.text.str.contains('|'.join(searchfor))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I removed the html code from the source column as potential data element to explore further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dict = {'<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>': 'Twitter for iPhone',\n",
    "'<a href=\"http://vine.co\" rel=\"nofollow\">Vine - Make a Scene</a>' : 'Vine - Make a Scene',\n",
    "'<a href=\"http://twitter.com\" rel=\"nofollow\">Twitter Web Client</a>' :'Twitter Web Client',\n",
    "'<a href=\"https://about.twitter.com/products/tweetdeck\" rel=\"nofollow\">TweetDeck</a>':'TweetDeck'}\n",
    "\n",
    "def clean_source(tw_image_clean):\n",
    "    if tw_image_clean['source'] in source_dict.keys():\n",
    "        sourceab = source_dict[tw_image_clean['source']]\n",
    "        return sourceab\n",
    "    else:\n",
    "        return tw_image_clean['source']\n",
    "    \n",
    "tw_image_clean['source'] = tw_image_clean.apply(clean_source, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I stored and exported the data for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
